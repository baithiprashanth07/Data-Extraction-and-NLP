# -*- coding: utf-8 -*-
"""Data Extraction and NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BuXLYFAKQje5uPsenLLInIJd82GMpshn

**Install Required Libraries**
"""

!pip install requests beautifulsoup4 openpyxl nltk textblob pyphen
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

"""**Data Loading**"""

import pandas as pd

input_file = '/content/Input.xlsx'
df = pd.read_excel(input_file)
urls = df['URL']
url_ids = df['URL_ID']

"""**Extract Article Text**"""

import requests
from bs4 import BeautifulSoup
import os

def extract_article_text(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    title = soup.find('title').get_text()
    article = soup.find('article')
    if article:
        paragraphs = article.find_all('p')
        article_text = ' '.join([para.get_text() for para in paragraphs])
    else:
        article_text = ''
    return title, article_text

output_dir = 'extracted_articles'
os.makedirs(output_dir, exist_ok=True)

for url_id, url in zip(url_ids, urls):
    title, text = extract_article_text(url)
    with open(os.path.join(output_dir, f'{url_id}.txt'), 'w', encoding='utf-8') as f:
        f.write(title + '\n' + text)

"""**Data Analysis**"""

import nltk
from textblob import TextBlob
import pyphen
import os



dic = pyphen.Pyphen(lang='en')

def count_syllables(word):
    syllables = dic.inserted(word)
    return len(syllables.split('-'))

def analyze_text(text):
    blob = TextBlob(text)
    sentences = blob.sentences
    words = blob.words

    # Positive and Negative Scores
    positive_words = ['good', 'great', 'best', 'fantastic']
    negative_words = ['bad', 'worst', 'awful']
    positive_score = sum(1 for word in words if word in positive_words)
    negative_score = sum(1 for word in words if word in negative_words)

    # Polarity and Subjectivity
    polarity_score = blob.sentiment.polarity
    subjectivity_score = blob.sentiment.subjectivity

    # Sentence and Word Analysis
    avg_sentence_length = sum(len(sentence.words) for sentence in sentences) / len(sentences)
    complex_words = [word for word in words if count_syllables(word) > 2]
    percentage_of_complex_words = len(complex_words) / len(words) * 100
    fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)
    avg_number_of_words_per_sentence = len(words) / len(sentences)
    complex_word_count = len(complex_words)
    word_count = len(words)
    syllable_per_word = sum(count_syllables(word) for word in words) / len(words)
    personal_pronouns = sum(1 for word in words if word.lower() in ['i', 'we', 'me', 'us', 'my', 'our', 'mine', 'ours'])
    avg_word_length = sum(len(word) for word in words) / len(words)

    return {
        'positive_score': positive_score,
        'negative_score': negative_score,
        'polarity_score': polarity_score,
        'subjectivity_score': subjectivity_score,
        'avg_sentence_length': avg_sentence_length,
        'percentage_of_complex_words': percentage_of_complex_words,
        'fog_index': fog_index,
        'avg_number_of_words_per_sentence': avg_number_of_words_per_sentence,
        'complex_word_count': complex_word_count,
        'word_count': word_count,
        'syllable_per_word': syllable_per_word,
        'personal_pronouns': personal_pronouns,
        'avg_word_length': avg_word_length,
    }

"""**Read and Analyze Articles:**"""

articles_dir = 'extracted_articles'
analysis_results = []

for url_id in url_ids:
    with open(os.path.join(articles_dir, f'{url_id}.txt'), 'r', encoding='utf-8') as f:
        text = f.read()
    analysis_result = analyze_text(text)
    analysis_result['URL_ID'] = url_id
    analysis_results.append(analysis_result)

"""**Save the Output**"""

output_file = 'Output Data Structure.xlsx'
output_df = pd.DataFrame(analysis_results)
output_df.to_excel(output_file, index=False)

